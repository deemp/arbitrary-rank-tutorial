\chapter{Design and Methodology}
\label{chap:DesignAndMethodology}

This chapter details the architectural design and core methodologies employed in the implementation of \texttt{Arralac}, a tutorial compiler for a lambda calculus with arbitrary-rank polymorphism. The design is heavily inspired by the bidirectional type inference system described in \textit{Practical type inference for arbitrary-rank types} \cite{jones-practical-2007}, but it incorporates several modern implementation techniques and diverges in key areas to prioritize clarity and extensibility. We will first outline the overall system architecture, then delve into the specifics of the Abstract Syntax Tree (AST) representation and the constraint-based type inference engine.

\section{System Architecture: The Compilation Pipeline}
\label{sec:Design:Pipeline}

The process of transforming a source file from plain text into an evaluated term is managed by a multi-stage pipeline. Each stage performs a distinct transformation on the program representation, passing its output to the next stage. This modular design isolates concerns, simplifies debugging, and facilitates future extensions. Its stages are described below.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
            node distance=1.5cm and 2.5cm,
            auto,
            block/.style={rectangle, draw, fill=blue!20, text width=7em, text centered, rounded corners, minimum height=3em},
            arrow/.style={-Latex, thick}
        ]
        % Define the pipeline stages as nodes
        \node[block] (reader) {Reading};
        \node[block, right=of reader] (parser) {Parsing};
        \node[block, right=of parser] (renamer) {Renaming};
        \node[block, below=of renamer] (typechecker) {Typechecking};
        \node[block, left=of typechecker] (solver) {Solving};
        \node[block, left=of solver] (zonker) {Zonking};
        \node[block, below=of zonker] (core) {Core Conversion};
        \node[block, right=of core] (evaluation) {Evaluation};

        % Draw the arrows connecting the stages
        \draw[arrow] (reader) -- (parser);
        \draw[arrow] (parser) -- (renamer);
        \draw[arrow] (renamer) -- (typechecker);
        \draw[arrow] (typechecker) -- (solver);
        \draw[arrow] (solver) -- (zonker);
        \draw[arrow] (zonker) -- (core);
        \draw[arrow] (core) -- (evaluation);
    \end{tikzpicture}
    \caption{The \texttt{Arralac} Pipeline}
    \label{fig:pipeline}
\end{figure}


\begin{itemize}
    \item \textbf{Reading:} The initial stage reads the source file from disk into a text buffer.
    \item \textbf{Parsing:} The text is passed to a parser generated by BNFC \cite{bnfc-site-2025}, which transforms the linear stream of characters into an initial Abstract Syntax Tree (AST). Each node in this tree is annotated with its location (span) in the source code.
    \item \textbf{Renaming:} This crucial stage performs $\alpha$-conversion by traversing the AST and assigning a unique identifier to every variable binding. This ensures that all occurrences of a variable are unambiguously linked to their binder, resolving issues of shadowing and preparing the program for typechecking.
    \item \textbf{Typechecking:} The renamed AST is traversed to generate a set of type constraints. This pass does not solve the constraints but gathers them, including equality constraints between types and implication constraints that capture the scoping requirements of polymorphism.
    \item \textbf{Solving:} A separate solver pass attempts to unify the generated constraints. It is here that skolem escape and occurs checks are performed. Unsolved metavariables are left as-is.
    \item \textbf{Zonking:} After solving, the AST is "zonked." This pass substitutes all solved metavariables with their inferred types, producing a fully-typed AST. Unsolved metavariables are explicitly marked as such.
    \item \textbf{Core Conversion \& Evaluation:} The final, zonked AST is converted into a simpler Core language representation for evaluation. The evaluator then reduces the Core term to its weak head normal form (WHNF).
\end{itemize}

\section{Abstract Syntax Tree Design}
\label{sec:Design:AST}

The choice of AST representation is a foundational design decision that profoundly impacts the compiler's extensibility and maintainability. My design choices were guided by several key technical constraints:
\begin{itemize}
    \item \textbf{Extensibility:} The AST needed to gracefully accommodate future language extensions, such as new literal types or statement forms, without requiring invasive changes to existing data type definitions.
    \item \textbf{Annotation for Tooling:} To support language server features, it was essential to annotate AST nodes with auxiliary information, particularly the source code span and, after typechecking, the inferred type of the node.
    \item \textbf{Parser Generator Integration:} The design had to integrate smoothly with the ASTs produced by the BNFC parser generator \cite{bnfc-site-2025}.
\end{itemize}

After evaluating several alternatives, including the default BNFC representation, \texttt{hypertypes} \cite{hypertypes-hackage}, and \texttt{compdata} \cite{compdata-hackage}, I selected the \textbf{Trees That Grow (TTG)} pattern \cite{trees-that-grow-2016}. TTG, also used within GHC \cite{ghc-gitlab-2025}, provides an elegant solution to the problem of extensibility by parameterizing data types by the compiler pass and using open type families for fields.

My implementation of TTG differs slightly from GHC's for improved flexibility. Whereas GHC uses concrete types in some fields, my implementation uses type family applications for \textit{all} fields, ensuring that every part of an AST node can be customized for a given compiler pass. This is illustrated in the comparison below.

\begin{minted}[frame=lines,label={GHC's TTG AST Structure (Simplified)}]{haskell}
-- In GHC (compiler/Language/Haskell/Syntax/Expr.hs)
data HsExpr p
  = HsVar (XVar p) (LIdP p) -- LIdP is a type synonym, not a type family
  ...

type LIdP p = XRec p (IdP p)
\end{minted}

\begin{minted}[frame=lines,label={Arralac's TTG AST Structure}]{haskell}
-- In Arralac (Language/Arralac/Syntax/TTG/SynTerm.hs)
data SynTerm x
  = SynTerm'Var (XSynTerm'Var' x) (XSynTerm'Var x) -- All fields use type families
  ...

-- Type families for each field, defined separately
type family XSynTerm'Var' x
type family XSynTerm'Var x
\end{minted}

This uniform use of type families allows, for instance, the type of a variable itself to change between passesâ€”from a simple \texttt{Name} in the `Renamed` pass to a \texttt{TcTermVar} containing a full type in the `Typechecked` pass.

\section{Type Inference and Constraint Solving}
\label{sec:Design:TypeInference}

The core of the compiler is its type inference engine. This section first reviews the foundational concepts from \cite{jones-practical-2007} that underpin the system, and then describes how \texttt{Arralac}'s implementation diverges to adopt a constraint-based approach.

\subsection{Foundations from \textit{Practical Type Inference}}
\label{sec:Design:Foundations}

The algorithm in \cite{jones-practical-2007} provides a practical way to handle higher-rank types by leveraging programmer annotations. Its key mechanisms are as follows:

\begin{itemize}
    \item \textbf{Type Hierarchy:} Types are stratified to manage polymorphism. \textbf{\texttt{tau} ($\tau$)} types are monotypes (no \texttt{forall}). \textbf{\texttt{rho} ($\rho$)} types can have nested polytypes but no top-level \texttt{forall}. \textbf{\texttt{sigma} ($\sigma$)} types are polytypes with a top-level \texttt{forall}. All interaction happens within a \textbf{type context} ($\Gamma$), which maps variables to their $\sigma$-types.

    \item \textbf{Metavariables and Unification:} Inference works by creating placeholder \textbf{metavariables} for unknown types and generating equality constraints. The process of solving these constraints is \textbf{unification}. Crucially, the paper's system operates under a \textbf{monotype invariant}: metavariables can only be unified with $\tau$-types. This predicative restriction is vital for ensuring that type inference is decidable and its result is \textbf{order-independent}.

    \item \textbf{Bidirectional Type Checking:} The algorithm operates in two modes. In \textbf{inference mode} ($\uparrow$), it synthesizes the most general type for an expression. In \textbf{checking mode} ($\downarrow$), it verifies that an expression conforms to a known, expected type. This duality is key: inference is used for simple expressions, but when a higher-rank type is expected (e.g., as a function argument), the system switches to checking mode, pushing the polymorphic type "down" into the expression and avoiding the need for undecidable full inference.

    \item \textbf{Subsumption and Skolemization:} To check if a function of type $\sigma_1$ can be used where $\sigma_2$ is expected (i.e., $\sigma_1$ is "more polymorphic than" $\sigma_2$), the system uses \textbf{subsumption}. This is implemented via \textbf{deep skolemization}, where the quantified variables of $\sigma_2$ are replaced with rigid, un-unifiable "skolem" constants. Then, $\sigma_1$ is instantiated (its quantified variables are replaced with fresh metavariables) and unified with the skolemized $\sigma_2$. The check succeeds if unification succeeds without a skolem escaping its scope.

    \item \textbf{Weak Prenex Form and Contravariance:} To handle nested quantifiers during subsumption, types are converted to a \textbf{weak prenex form}, lifting inner quantifiers to the top level. For function types, subsumption is \textbf{contravariant} in the argument position: to check $(\sigma_1 \to \rho_1) \le (\sigma_2 \to \rho_2)$, the algorithm must verify $\sigma_2 \le \sigma_1$ and $\rho_1 \le \rho_2$.
\end{itemize}

\subsection{The \texttt{Arralac} Approach: Constraint-Based Inference}
\label{sec:Design:ArralacApproach}

While the theoretical underpinnings of \texttt{Arralac} are rooted in \cite{jones-practical-2007}, the implementation of the inference mechanism diverges significantly. Instead of the \textit{eager unification} described in the paper, where constraints are solved as they are discovered, \texttt{Arralac} adopts a \textit{constraint-based} approach inspired by the architecture of GHC \cite{ghc-aosabook-2025, wits-type-inference-using-constraints}.

\paragraph{Constraint Generation and Solving.} The typechecking process is split into two distinct phases: generation and solving. The typechecker traverses the renamed AST and produces a collection of "wanted" constraints but does not attempt to solve them. This includes two main types of constraints:
\begin{enumerate}
    \item \textbf{Equality Constraints (\texttt{CEqCan}):} These represent a required equality between a metavariable and a type.
    \item \textbf{Implication Constraints (\texttt{Implic}):} When skolemization occurs (e.g., checking an expression against a polymorphic type), an implication constraint is generated. This constraint bundles the skolem variables with a new set of "wanted" constraints that are generated from checking the body of the expression within the scope of the skolems.
\end{enumerate}

This separation allows the entire program to be analyzed before any unification occurs, which can lead to better error reporting. The collected constraints are then passed to a dedicated solver.

\paragraph{Level-Based Skolem Escape Checking.} Instead of a global type context, \texttt{Arralac} manages polymorphism scoping using \textbf{levels}, a technique also used in modern compilers \cite{practical-type-inference-with-levels-2025}. Every bound, skolem, and meta variable is assigned an integer \texttt{TcLevel} at its creation time. The level is incremented upon entering a skolemization scope (i.e., an implication constraint). The solver then enforces the skolem escape check by a simple rule: a metavariable at level $n$ cannot be unified with a type containing any skolem variable from a level $m > n$. This check is performed during the solving pass.

\paragraph{Occurs Check and Finalization.} During solving, the solver also performs an \textbf{occurs check} to ensure that a metavariable does not appear in the type it is being unified with, preventing infinite types. After the solver runs, a final \textbf{zonking} pass replaces all solved metavariables in the AST. Any metavariables that remain unsolved are left in the final AST, explicitly marked as unsolved.

\section{Summary of Design Choices and Limitations}
\label{sec:Design:Summary}

The design of \texttt{Arralac} makes several deliberate trade-offs to prioritize its tutorial nature and extensibility over feature completeness.
The key design choices were:
\begin{itemize}
    \item An extensible \textbf{Trees That Grow} AST to support future language features and tooling annotations.
    \item A GHC-style, two-phase \textbf{constraint-based type inference} engine, which separates constraint generation from solving.
    \item \textbf{Level-based scoping} for skolem variables, providing an efficient mechanism for escape checking.
\end{itemize}

This design, however, comes with several limitations compared to a production compiler like GHC or even the full system described in \cite{jones-practical-2007}. The accompanying Technical Appendix to the paper \cite{practical-type-inference-proofs} provides proofs of soundness and completeness for the theoretical system, but this implementation does not attempt to formally prove its own correctness. The most notable limitations are:
\begin{itemize}
    \item \textbf{No \texttt{let}-generalization:} Local \texttt{let}-bindings are not generalized, a simplification suggested in \cite{wits-type-inference-using-constraints}.
    \item \textbf{No recursive \texttt{let}-bindings.}
    \item \textbf{No floating-out of constraints:} The solver does not attempt to move equality constraints out of implications to enable further solving.
    \item \textbf{Untyped Core Language:} Unlike GHC, \texttt{Arralac} translates to a simple, untyped Core language, forgoing the powerful consistency checks that a typed intermediate language provides.
    \item \textbf{Simplified Constraint Solving:} The solver uses a simple worklist and does not handle residual constraints or complex interactions, dropping any constraints that it cannot immediately solve.
\end{itemize}
