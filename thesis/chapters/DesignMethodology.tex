\chapter{Design and Methodology}
\label{chap:DesignAndMethodology}

This chapter details the architectural design and core methodologies employed in the implementation of \Arralac, a tutorial compiler for a lambda calculus with arbitrary-rank polymorphism. The design is heavily inspired by the bidirectional type inference system described in \textit{Practical type inference for arbitrary-rank types} \cite{jones-practical-2007}, but it incorporates several modern implementation techniques and diverges in key areas to prioritize clarity and extensibility. This chapter will first outline the overall system architecture, then justify the choice of Abstract Syntax Tree (AST) representation, and finally, delve into the specifics of the constraint-based type inference engine and the methodology used to validate its correctness.

\section[System Architecture: The Compilation Pipeline]{System Architecture: \\ The Compilation Pipeline}
\label{sec:Design:Pipeline}

The process of transforming a source file from plain text into an evaluated term is managed by a multi-stage pipeline, depicted in \Cref{fig:pipeline}. Each stage performs a distinct transformation on the program representation, passing its output to the next. This standard pipeline structure \cite{wits-type-inference-using-constraints} was chosen to promote modularity and a clear separation of concerns, which are essential for creating an understandable and maintainable tutorial compiler.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
            node distance=1.5cm and 2.5cm,
            auto,
            block/.style={rectangle, draw, fill=blue!20, text width=7em, text centered, rounded corners, minimum height=3em},
            arrow/.style={-Latex, thick}
        ]
        % Define the pipeline stages as nodes
        \node[block] (reader) {Reading};
        \node[block, right=of reader] (parser) {Parsing};
        \node[block, right=of parser] (renamer) {Renaming};
        \node[block, below=of renamer] (typechecker) {Typechecking};
        \node[block, left=of typechecker] (solver) {Solving};
        \node[block, left=of solver] (zonker) {Zonking};
        \node[block, below=of zonker] (core) {Core Conversion};
        \node[block, right=of core] (evaluation) {Evaluation};

        % Draw the arrows connecting the stages
        \draw[arrow] (reader) -> (parser);
        \draw[arrow] (parser) -> (renamer);
        \draw[arrow] (renamer) -> (typechecker);
        \draw[arrow] (typechecker) -> (solver);
        \draw[arrow] (solver) -> (zonker);
        \draw[arrow] (zonker) -> (core);
        \draw[arrow] (core) -> (evaluation);
    \end{tikzpicture}
    \caption{The \Arralac Compilation Pipeline.}
    \label{fig:pipeline}
\end{figure}

The stages are as follows:
\begin{itemize}
    \item \textbf{Reading \& Parsing:} The source text is read and transformed by a BNFC-generated parser \cite{bnfc-site-2025} into an initial AST, with each node annotated with its source location.
    \item \textbf{Renaming:} This stage performs $\alpha$-conversion, assigning a unique identifier to every variable binding to resolve shadowing and prepare the program for typechecking.
    \item \textbf{Typechecking (Constraint Generation):} The renamed AST is traversed to generate a set of type constraints. This pass gathers equality and implication constraints but does not solve them.
    \item \textbf{Solving:} A separate iterative solver pass attempts to unify variables with types in the generated constraints, performing skolem escape and occurs checks.
    \item \textbf{Zonking:} After solving, a final pass substitutes all solved metavariables with their inferred types, producing a fully-typed AST.
    \item \textbf{Core Conversion:} The final, zonked AST is converted into a simpler Core language.
    \item \textbf{Evaluation:} The term in that Core language is evaluated to its weak head normal form (WHNF).
\end{itemize}

\section{Abstract Syntax Tree Design}
\label{sec:Design:AST}

The choice of AST representation is a foundational design decision. My design was guided by several key technical constraints: extensibility for future language features, the ability to annotate nodes for tooling, and smooth integration with the BNFC parser generator.

\subsection{Evaluating AST Representation Strategies}
Several alternatives were considered to meet these constraints. While the default AST generated by BNFC is simple and integrates perfectly with the parser, it lacks the necessary structural flexibility for a multi-pass compiler because it is too close to the concrete language syntax.

% TODO TTG uses type families
Libraries like \texttt{hypertypes} \cite{hypertypes-hackage} and \texttt{compdata} \cite{compdata-hackage} offer powerful approaches to building extensible ASTs, serving as a solution to the Expression Problem \cite{wadler-expression-1998}. However, they introduce a significant degree of a conceptual overhead that was deemed counterproductive for a tutorial project aimed at clarifying compiler internals.

Therefore, the \textbf{Trees That Grow (TTG)} pattern was selected \cite{trees-that-grow-2016}. It strikes an ideal balance, offering the extensibility needed for a phased compiler while remaining conceptually straightforward. It directly models the evolution of the AST through different stages, making it a powerful pedagogical tool in its own right and mirroring the architecture of GHC \cite{ghc-gitlab-2025}.


% TODO no extension field

\enlargethispage{\baselineskip}
My implementation of TTG differs slightly from GHC's. Whereas GHC uses concrete types in some fields, my implementation uses type family applications for \textit{all} fields, ensuring that every part of an AST node can be customized for a given compiler pass. Another difference is that I did not yet introduce an additional constructor for extending data types defined in the TTG representation because I did not expect usage of these types outside of the \Arralac compiler.

\begin{figure}
    \centering
    \begin{minted}[frame=lines]{haskell}
    -- In GHC (compiler/Language/Haskell/Syntax/Expr.hs)
    data HsExpr p
      = HsVar (XVar p) (LIdP p) -- LIdP is a type synonym, not a type family
      ...
    
    type LIdP p = XRec p (IdP p)
    \end{minted}
    \caption{GHC's TTG AST Structure (Simplified)}
\end{figure}

\begin{figure}
    \centering
    \begin{minted}[frame=lines]{haskell}
    -- In Arralac (Language/Arralac/Syntax/TTG/SynTerm.hs)
    data SynTerm x
      = SynTerm'Var (XSynTerm'Var' x) (XSynTerm'Var x) -- All fields use type families
      ...
    
    -- Type families for each field, defined separately
    type family XSynTerm'Var' x
    type family XSynTerm'Var x
    \end{minted}
    \caption{Arralac's TTG AST Structure}
\end{figure}

This uniform use of type families provides a type-safe guarantee that pass-specific information, such as inferred types, is only available in the AST after that pass has successfully completed.

\section{Type Inference and Constraint Solving}
\label{sec:Design:TypeInference}

The design of the \Arralac typechecker required a concrete implementation of several core concepts from the bidirectional system of \cite{jones-practical-2007}. This section outlines these foundational mechanisms that the methodology must realize and then describes the significant architectural divergence taken in \Arralac.

\subsection{Foundations from \textit{Practical Type Inference}}
\label{sec:Design:Foundations}

The design of \Arralac's typechecker, while architecturally distinct, is a direct evolution of the bidirectional system presented in \textit{Practical type inference for arbitrary-rank types} \cite{jones-practical-2007}. To fully appreciate the design choices made in \Arralac, it is essential to first understand the foundational mechanisms of this influential system, which elegantly balances expressive power with practical, decidable inference.

\subsubsection{Type Hierarchy and Context.}
At its core, the system stratifies types into a precise hierarchy to manage polymorphism. The type context, denoted by \textbf{$\Gamma$}, maps program variables to their types.
\begin{itemize}
    \item \textbf{Monotypes ($\tau$-types):} These are simple types with no polymorphism whatsoever, such as \texttt{Int} or \texttt{Bool -> Int}. They form the bedrock of the system.
    \item \textbf{Polytypes ($\sigma$-types):} These are types that begin with a \texttt{forall} quantifier, such as \texttt{forall a. a -> a}. They introduce polymorphism.
    \item \textbf{Rho-types ($\rho$-types):} These are an intermediate category, representing types that can appear on the right-hand side of a function arrow or under a \texttt{forall}. In the full higher-rank system, a $\rho$-type can be either a $\tau$-type or a function type $\sigma_1 \rightarrow \sigma_2$.
\end{itemize}
This stratification is crucial for controlling where and how polymorphism can appear and be inferred.

\subsubsection{Metavariables and the Monotype Invariant.}
During inference, the typechecker encounters expressions whose types are not yet known. It handles this by creating placeholder types called \textbf{metavariables} (e.g., $\alpha$, \texttt{$\beta$}). These are temporary, flexible variables that are gradually refined as the algorithm gathers more information. The process of solving for these metavariables is called \textbf{unification}.

A cornerstone of the system's practicality is the \textbf{monotype invariant}: a metavariable can only ever be unified with a monotype ($\tau$-type). This restriction is what guarantees that type inference is decidable and, crucially, \textbf{order-independent}. If a metavariable could be unified with a polytype, the result of inference could depend on the order in which expressions were traversed, leading to unpredictable and non-deterministic behavior. By enforcing this invariant, the algorithm ensures that the principal type it infers is unique and consistent.

\subsubsection{The Bidirectional Algorithm.}
The master strategy for handling higher-rank types is the \textbf{bidirectional type checking} algorithm. Instead of a single, monolithic inference function, the system operates in two distinct modes, preventing the need for full, undecidable inference:
\begin{itemize}
    \item \textbf{Inference Mode ($\uparrow$):} This mode synthesizes or "infers" the most general type for an expression from the bottom up. In the paper's implementation, this corresponds to functions like \texttt{inferRho} \\ and \texttt{inferSigma}. It is used for expressions where the type is not already known.
    \item \textbf{Checking Mode ($\downarrow$):} This mode verifies or "checks" an expression against a known, expected type that is pushed down from the surrounding context. This is implemented by functions like \texttt{checkRho} and \texttt{checkSigma}.
\end{itemize}
This duality is key. For most of the program, the system can infer types. However, when it encounters a programmer-supplied annotation for a higher-rank type (e.g., on a function argument), it switches to the more constrained checking mode, using the annotation to guide the process and avoid intractable search.

\subsubsection{Subsumption: The "More Polymorphic Than" Check.}
A central challenge is determining if a function can be used where a more-specific polymorphic type is expected. This is handled by a \textbf{subsumption} check, which formalizes the notion of one type being "more polymorphic than" another. In the paper's implementation, this is the role of the \texttt{subsCheck} function. The mechanics of subsumption rely on two dual concepts: instantiation and skolemization.

% TODO clarify using, bound, checking against. What is the relation to programmer-given signatures?
\begin{itemize}
    \item \textbf{Instantiation:} When \textit{using} a polymorphic value (e.g., calling a function bound to a $\sigma$-type), its quantified type variables are replaced with fresh metavariables. This process, handled by \texttt{instantiate}, makes the type usable in a specific context.

    \item \textbf{Deep Skolemization:} The more complex case is checking against a polymorphic type \texttt{forall a. T}. To do this, the algorithm replaces the quantified variable \texttt{a} with a new, rigid constant called a \textbf{skolem}. A skolem constant is unique and cannot be unified with any other type. If a skolem constant "escapes" its scope (\textbf{skolem escape}) during subsequent unification (e.g., by being unified with a metavariable from an outer scope), a type error is raised.

          For arbitrary-rank types like \texttt{T1 -> (forall a. T2)}, the \texttt{forall} is nested. \textbf{Deep skolemization} is the paper's crucial innovation to handle this. It first transforms the type into a \textbf{weak prenex form} using an auxiliary function, $pr(\sigma)$, which pulls nested quantifiers from the return types of functions to the outermost level. For example,

          \begin{figure}
              \begin{equation*}
                  \begin{split}
                      pr(\forall c. (\forall a. a \rightarrow c) \rightarrow (\forall b. b \rightarrow c)) & = \\
                      \quad \quad =\forall b c. \rightarrow (\forall a. a \rightarrow c) \rightarrow b \rightarrow c
                  \end{split}
              \end{equation*}
              \caption{Transforming a polytype into a weak-prenex form}
          \end{figure}

          This makes all quantifiers accessible for skolemization, enabling a consistent and robust subsumption check.
\end{itemize}

Finally, when checking function types for subsumption, such as \\ \texttt{($\sigma_1~\rightarrow~\sigma_2)~\leq~(\sigma_3~\rightarrow~\sigma_4$)}, the algorithm respects \textbf{contravariance}. The argument types are checked in the opposite direction ($\sigma_3 \leq \sigma_1$), while the result types are checked in the same direction ($\sigma_2 \leq \sigma_4$).

These mechanisms --- a strict type hierarchy, the monotype invariant, bidirectional checking, and subsumption check via deep skolemization --- work in concert to create a system that is powerful enough to support higher-rank types yet constrained enough to remain practical and decidable. This is the foundational system that \Arralac reinterprets through a modern, constraint-based lens.

\subsection[Architectural Divergence: A Constraint-Based Model]{Architectural Divergence: \\ A Constraint-Based Model}
\label{sec:Design:ArralacApproach}

While the theoretical underpinnings are rooted in \cite{jones-practical-2007}, the implementation of the inference mechanism diverges significantly. Instead of the \textbf{eager unification} described in the paper, where constraints are solved as they are discovered, \Arralac adopts a \textbf{constraint-based} approach inspired by GHC. This architectural choice was made to enhance modularity and to lay a more robust foundation for high-quality error diagnostics, reflecting modern compiler practice. \Cref{tab:arch-comparison} summarizes the key differences.

\begin{table}[h!]
    \centering
    \small
    \caption{Comparison of Type Inference Architectures}
    \label{tab:arch-comparison}
    \begin{tabular}{p{0.2\textwidth} p{0.35\textwidth} p{0.35\textwidth}}
        \toprule
        \textbf{Feature}            & \textbf{\cite{jones-practical-2007} (Eager Unification)}                             & \textbf{                                                                                                                                                                  \Arralac (Constraint-Based)} \\
        \midrule
        \textbf{Unification}        & Solves constraints immediately as they are generated.                                & Gathers all constraints first; solves them in a separate, dedicated pass.                                                                                                                              \\
        \textbf{Modularity}         & Inference logic is tightly coupled with unification logic.                           & The Typechecker (generation) and Solver (unification) are fully decoupled modules.                                                                                                                     \\
        \textbf{Error Reporting}    & Errors are reported at the first point of unification failure, which can be obscure. & Already reports the location of a sub-term where a constraint originated. Has the potential for holistic error analysis by examining all conflicting constraints at once.                              \\
        \textbf{Let-Generalization} & Requires a global type context traversal at the point of the \texttt{let}-binding.   & Not implemented, but the architecture permits solving of scoped constraints during type checking.                                                                                                      \\
        \bottomrule
    \end{tabular}
\end{table}

\newpage
This separation is realized through two primary constraint types:
\begin{enumerate}
    \item \textbf{Canonical Equality Constraints (\texttt{EqCt}):} Represent a required equality between a metavariable and a type.
    \item \textbf{Implication Constraints (\texttt{Implication}):} Capture the scoping of polymorphism. An implication bundles a set of skolem variables created at the ambient level with \textbf{wanted} \cite{wits-type-inference-using-constraints} constraints generated from checking an expression after incrementing the ambient level.
\end{enumerate}

Furthermore, \Arralac manages polymorphism scoping using \textbf{levels}, a technique also used in modern compilers \cite{practical-type-inference-with-levels-2025}. Every variable is assigned an integer \texttt{TcLevel} at creation. The solver then enforces the skolem escape check by a simple rule: a metavariable at level $n$ cannot be unified with a type containing any free variable (skolem or a metavariable) from a level $m > n$ \footnote{See \href{https://github.com/ghc/ghc/blob/ed38c09bd89307a7d3f219e1965a0d9743d0ca73/compiler/GHC/Tc/Utils/Unify.hs\#L2589}{Note [Unification preconditions]}}.

\section{Validation Methodology}
\label{sec:Implementation:Methodology}

% TODO generate test cases
To validate the correctness of the implementation and ensure it meets its pedagogical goals, a methodology of targeted, feature-driven testing was employed. Rather than relying on a large, undifferentiated test suite, specific test cases were developed to exercise the core mechanisms of the arbitrary-rank type system and its implementation.

\begin{enumerate}
    \item \textbf{Correct Handling of Higher-Rank Polymorphism:} The primary validation case involved a program that requires passing a polymorphic function as an argument, similar to \texttt{Program1.arralac} (\cref{sec:Implementation:Results}). The successful typechecking and evaluation of this program served as the baseline validation that the core bidirectional algorithm, including subsumption and deep skolemization, was implemented correctly.

          % TODO compose this test
    \item \textbf{Skolem Escape and Level Checking:} A specific negative test case was created to ensure that a skolem variable could not escape its scope during unification. The test involved attempting to unify a metavariable with a type containing a skolem from a deeper scope. The expected outcome was a type error from the solver, which validated that the level-based checking mechanism was correctly preventing unsound unifications.

    \item \textbf{Language Server Functionality:} The interactive tooling was validated manually within Visual Studio Code. This involved checking that (a) hovering over identifiers displayed the correct, fully-zonked types as inferred by the pipeline, and (b) introducing syntactic or semantic errors (e.g., unbound variables) triggered appropriate and timely error diagnostics from the language server.
\end{enumerate}

While this methodology does not constitute a formal proof of correctness, it provides enough evidence that the key features of the system are implemented correctly and robustly, satisfying the primary objectives of the thesis.

\section[Summary of Design Choices and Limitations]{Summary of Design Choices \\ and Limitations}
\label{sec:Design:Summary}

The design of \Arralac makes several deliberate trade-offs to prioritize its tutorial nature and extensibility over feature completeness. The key design choices were:
\begin{itemize}
    \item An extensible \textbf{Trees That Grow} AST to support future language features and tooling annotations.
    \item A GHC-style, two-phase \textbf{constraint-based type inference} engine, which separates constraint generation from solving.
    \item \textbf{Level-based scoping} for skolem variables, providing an efficient mechanism for escape checking.
\end{itemize}

This design, however, comes with several limitations compared to a production compiler. The accompanying Technical Appendix to the paper \cite{practical-type-inference-proofs} provides proofs of soundness and completeness for the theoretical system, but this implementation does not attempt to formally prove its own correctness. The most notable limitations are:
\begin{itemize}
    \item \textbf{No \texttt{let}-generalization:} Local \texttt{let}-bindings are not generalized, a simplification suggested in \cite{vytiniotis-outsideinx-2011}.
    \item \textbf{No recursive \texttt{let}-bindings.} just like in \cite{jones-practical-2007}.
    \item \textbf{No floating-out of constraints:} The solver does not attempt to move equality constraints out of implications to enable further solving.
    \item \textbf{Untyped Core Language:} Unlike GHC and \cite{jones-practical-2007}, \Arralac translates \texttt{SynTerm}s to a simple, untyped Core language, forgoing the powerful consistency checks that a typed intermediate language provides. This simplification was a deliberate trade-off to keep the focus of the thesis squarely on the front-end type inference algorithm.
    \item \textbf{Simplified Constraint Solving:} The solver does not rewrite Wanteds with Wanteds \footnote{See \href{https://github.com/ghc/ghc/blob/ed38c09bd89307a7d3f219e1965a0d9743d0ca73/compiler/GHC/Tc/Types/Constraint.hs\#L2415}{Note [Wanteds rewrite Wanteds]}} and only reports the first constraint that it could not solve, not all residual constraints.
\end{itemize}