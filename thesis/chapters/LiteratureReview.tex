\chapter{Literature Review}
\label{chap:LiteratureReview}

This chapter reviews several abstract syntax tree (AST) representations (\cref{sec:AstRepresentations}) and type inference algorithms (\cref{sec:TypeInferenceAlgorithm}). This review aimed to identify approaches suitable for different aspects of our implementation, considering the technical constraints outlined below.

\section{Technical Constraints}
\label{chap:LiteratureReview:sec:AstRepresentations:TechnicalConstraints}

Our design and implementation choices were guided by the following technical constraints and requirements:

\begin{itemize}
  \item \textbf{Extensibility:} We anticipated future extensions to the language, including support for modules, integer and floating-point numbers, and record types. The chosen AST representation should accommodate such additions gracefully.
  \item \textbf{Mutual Recursion:} A requirement was the ability to employ mutually recursive data types for different categories of AST nodes. For instance, categories like modules and statements might be mutually dependent: a module could contain statements, and a statement could potentially introduce a nested module.
  \item \textbf{Annotation for Tooling:} The language server necessitated annotating AST nodes with auxiliary information. This includes the source code location (span) corresponding to each node and the inferred type of the expression represented by a node's subtree.
  \item \textbf{Parser Generator Integration:} We utilized the BNFC parser generator. BNFC produces an AST data type parameterized by an annotation type variable. Post-parsing, the annotation for most AST nodes includes the source code span from which the node was derived.
        % TODO note about span details can remain a comment or be integrated if relevant now.
        % Original TODO: not only start position - see https://github.com/BNFC/bnfc/pull/463
\end{itemize}

\section{AST Representations}
\label{sec:AstRepresentations}

Several AST representation techniques were considered, including the default data types generated by BNFC (\cref{chap:LiteratureReview:sec:AstRepresentations:BnfcAst}), the \texttt{hypertypes} library and related approaches (\cref{chap:LiteratureReview:sec:AstRepresentations:Hypertypes}), the \texttt{compdata} library (\cref{chap:LiteratureReview:sec:AstRepresentations:Compdata}), the Stitch representation (\cref{chap:LiteratureReview:sec:AstRepresentations:Stitch}), the Free Foil technique (\cref{chap:LiteratureReview:sec:AstRepresentations:FreeFoil}), and the Trees That Grow pattern (\cref{chap:LiteratureReview:sec:AstRepresentations:TreesThatGrow}).

% TODO Evaluate approaches?

\subsection{BNFC AST}
\label{chap:LiteratureReview:sec:AstRepresentations:BnfcAst}

The parsers generated by BNFC do not directly support parsing signed integer literals. A common workaround involves defining a custom \texttt{token} for the pattern, parsing it into a node containing the raw string representation, and subsequently post-processing the generated AST to convert these string-based nodes into nodes containing the actual numeric values using an \texttt{internal} (non-parsable) constructor.

\begin{minted}{haskell}
token IntegerSigned ('-'? digit+) ;
-- Node holding the raw string
LitIntegerRaw. Object ::= IntegerSigned ;
-- Node holding the parsed Integer
internal LitInteger. Object ::= IntegerSigned ;
\end{minted}

A drawback of this approach is the persistence of both node variants (\texttt{LitIntegerRaw} and \texttt{LitInteger}) within the AST data type definition. This requires handling the \texttt{LitIntegerRaw} case in pattern matching, even though it becomes redundant after the post-processing step.

\subsection{\texttt{hypertypes}}
\label{chap:LiteratureReview:sec:AstRepresentations:Hypertypes}

The \texttt{hypertypes} package \cite{hypertypes-hackage} enables the construction of expressions from individual components, reminiscent of the Data types à la carte approach \cite{swierstra-data-2008}. These components can represent mutually recursive types, similar to \texttt{multirec} \cite{multirec-hackage}, and are processed using type classes. The package documentation discusses the limitations of several preceding approaches.

Key features include primitives for node annotation (\href{https://hackage.haskell.org/package/hypertypes-0.2.2/docs/Hyper-Combinator-Ann.html}{\texttt{Hyper.Ann}}), constructing typed lambda calculus expressions (\href{https://hackage.haskell.org/package/hypertypes-0.2.2/docs/Hyper-Syntax.html}{\texttt{Hyper.Syntax}}), and unification (\href{https://hackage.haskell.org/package/hypertypes-0.2.2/docs/Hyper-Unify.html}{\texttt{Hyper.Unify}}). The \href{https://github.com/lamdu/hypertypes/blob/06cf48ef9c85c54cbe722a448754cb89931b23e7/src/Hyper/Diff.hs}{\texttt{Hyper.Diff}} module demonstrates tree annotation, while the \href{https://github.com/lamdu/hypertypes/tree/06cf48ef9c85c54cbe722a448754cb89931b23e7/test/TypeLang.hs}{\texttt{TypeLang}} example provides a type inference implementation for a language with row-types, utilizing the package's primitives.

\subsection{\texttt{compdata}}
\label{chap:LiteratureReview:sec:AstRepresentations:Compdata}

Similarly, the \texttt{compdata} package \cite{compdata-hackage} supports mutually recursive data types, including GADTs, via its \href{https://hackage.haskell.org/package/compdata-0.13.1/docs/Data-Comp-Multi.html}{\texttt{Data.Comp.Multi}} module. Available examples demonstrate the use of annotated ASTs within this framework (\href{https://github.com/pa-ba/compdata/blob/e916a9ae847b37d7932669f9365de987d09fd9e0/src/Data/Comp/Multi.hs#L322}{example1}, \href{https://github.com/pa-ba/compdata/blob/e916a9ae847b37d7932669f9365de987d09fd9e0/examples/Examples/Multi/Desugar.hs}{example2}, \href{https://gist.github.com/liarokapisv/bb857a23ecd9df945690f73e0acfbe80}{example3} related to the \href{https://github.com/pa-ba/compdata/issues/35}{Issue \#35}).

\subsection{Stitch}
\label{chap:LiteratureReview:sec:AstRepresentations:Stitch}

\citeauthor{eisenberg-stitch-2020} \cite{eisenberg-stitch-2020} explored an implementation of a simply typed $\lambda$-calculus. This work featured a non-typechecked AST employing type-level de Bruijn indices to ensure the construction of only well-scoped terms during parsing, alongside a type-checked AST indexed by type-level contexts and node types. The implementation heavily utilizes advanced Haskell extensions, serving as a practical case study for their application.

\subsection{Free Foil}
\label{chap:LiteratureReview:sec:AstRepresentations:FreeFoil}

The Free Foil approach \cite{kudasov-free-2024} by \citeauthor{kudasov-free-2024}, implemented in the \cite{free-foil-hackage} package, allows for constructing ASTs where nodes are indexed by a phantom type variable representing the scope. Nodes represent either variables or other language constructs. Constructs can be scoped under a (single) binder that extends the scope associated with the phantom type variable. This structure enables type-safe, capture-avoiding substitution of variables for expressions. Furthermore, it supports the definition of generic recursive AST processing functions applicable to any AST whose node signatures (\texttt{sig}) implement required type classes, such as \texttt{Bifunctor}.

The example in \href{https://hackage.haskell.org/package/free-foil-0.2.0/docs/Control-Monad-Free-Foil-Example.html}{\texttt{Control.Monad.Free.Foil.Example}} demonstrates defining an AST for an untyped lambda calculus, using the \href{https://hackage.haskell.org/package/free-foil-0.2.0/docs/src/Control.Monad.Free.Foil.Example.html#LamE}{\texttt{LamE}} pattern synonym to construct expressions under a binder.

Initially, we considered using the Free Foil representation of our AST to track bound variables. However, we later found out that the Free Foil approach presented two potential limitations relative to our technical constraints (\cref{chap:LiteratureReview:sec:AstRepresentations:TechnicalConstraints}):

First, the current library version requires mutually recursive types within the AST to be combined into a single sum data type acting as the signature \texttt{sig}. The library author notes that supporting truly separate mutually recursive types might be theoretically possible using a different internal representation.

% TODO link to the definition of lexical scoping
Second, Free Foil might increase the AST complexity or size in the version of PLC featuring modules or other forms of non-lexical scoping. For instance, in a language with modules, correctly relating variable declarations to their usage sites might necessitate multi-phase analysis (e.g., using scope graphs \cite{poulsen-monadic-2023}). If binders are used to track declaration sites, one might need to: (1) parse into an initial AST without resolved binders, (2) perform scope analysis, and (3) construct the second AST with appropriate binders reflecting the resolved scopes. For module imports, this could involve adding nodes within the import's subtree that introduce binders for all imported symbols, while simultaneously needing to retain information about the originating module for each binder.

\subsection{Trees That Grow}
\label{chap:LiteratureReview:sec:AstRepresentations:TreesThatGrow}

Thе Trees That Grow approach involves parameterizing data types (like AST nodes) with a type variable, often representing a compilation phase or state. Instead of concrete types, constructor fields use open type families applied to this parameter. This allows tailoring the AST structure for different phases by defining distinct instances of these type families.

For extensibility, data types typically include an "extension constructor" whose payload field is also defined via a type family. This allows "adding" new constructors post-definition by resolving this field's type to a new data type and providing pattern synonyms that wrap the new constructors, making them appear as part of the original type. A similar technique, using extra fields defined by type families within existing constructors, allows "adding" fields to those constructors.

Unlike the current Free Foil implementation, Trees That Grow readily supports mutually recursive data types. Its usage pattern resembles employing standard parameterized algebraic data types, although it necessitates defining a significant number of type family instances for constructor fields.

GHC utilizes a variant of the Trees That Grow pattern \cite{trees-that-grow-2016} for its internal AST representation (\href{https://gitlab.haskell.org/ghc/ghc/-/wikis/implementing-trees-that-grow/hs-syn}{\texttt{HsSyn}}, \href{https://gitlab.haskell.org/ghc/ghc/-/wikis/implementing-trees-that-grow/trees-that-grow-guidance}{Trees That Grow Guidance}). The GHC AST is parameterized by the compilation phase \href{https://github.com/ghc/ghc/blob/ed38c09bd89307a7d3f219e1965a0d9743d0ca73/compiler/GHC/Hs/Extension.hs#L169}{\texttt{Pass}} (\texttt{Parsed}, \texttt{Renamed}, \texttt{Typechecked}), using type families to enable/disable specific constructors or select appropriate annotation types for fields in each phase.

\section{Type Inference Algorithms}
\label{sec:TypeInferenceAlgorithm}

We reviewed several approaches to typing, ranging from the established system used in GHC to more recent algorithmic developments. We primarily focused on the bidirectional algorithms as they were said to produce better error messages due to error locality \cite{dunfield-bidirectional-2020}.

\subsection{GHC's Type Inference}

The GHC type inference engine, comprising approximately 50000 lines of code \cite{jones-typechecker-2023}, has evolved incrementally over many years to support numerous extensions to the Haskell type system. Many of these extensions were accompanied by scientific publications.

One foundational extension is \href{https://gitlab.haskell.org/haskell/prime/-/wikis/RankNTypes}{\texttt{RankNTypes}}. This extension enables parametric predicative arbitrary-rank polymorphism using a version of the bidirectional typing algorithm described in \cite{jones-practical-2007}.

\subsection{Developments Beyond GHC}

Since the publication of \cite{jones-practical-2007}, research has yielded multiple new type inference algorithms distinct from GHC's evolution. Several of these have publicly available Haskell implementations \cite{github-goldenberg-artem-goldenbergbidirectionalsystem-2025, github-choi-kwanghoonbidi-2025, github-chen-cu1ch3ntype-inference-zoo-2025}. The following subsections highlight some key developments.
% TODO Consider adding links to implementations if appropriate for the thesis format.

\subsubsection{Bidirectional Typing}

Bidirectional typing systems typically operate in two modes: an inference mode, that determines the type of a program construct and helps reduce the need for explicit type annotations, and a checking mode, which verifies top-down whether a program construct conforms to an expected type \cite{dunfield-bidirectional-2020}. This second mode allows typing constructs (like lambda abstractions without annotated arguments in certain contexts) for which types cannot be uniquely inferred.

\cite{dunfield-complete-2013} presented a relatively simple bidirectional type inference algorithm for systems with higher-rank predicative polymorphism. Their algorithm exhibits properties similar \cite[Fig. 15]{dunfield-complete-2013} to those described in \cite[Sec.~6]{jones-practical-2007}, including adherence to the $\eta$-law \cite[Ch.~4]{selinger-lecture-2013} and soundness and completeness with respect to System F \cite[Ch.~8]{selinger-lecture-2013}. The authors argue that their formulation, using ordered contexts, existential variables, and precise scoping rules, offers a better type-theoretic foundation compared to the "bag of constraints," unification variables, and skolemization techniques employed in earlier work like \cite{jones-practical-2007}.

Building on this, \citeauthor{dunfield-sound-2019} \cite{dunfield-sound-2019} extended the approach to a significantly richer language featuring existential quantification, sums, products, and pattern matching. Their formal development utilizes a desugared core language \cite[Fig. 11]{dunfield-sound-2019} derived from a more user-friendly surface language \cite[Fig. 1]{dunfield-sound-2019}.

Furthermore, \cite{dunfield-bidirectional-2020} provide an extensive survey of bidirectional typing techniques and offer practical guidance for designing new, programmer-friendly bidirectional type systems.

\subsubsection{Modifications and Generalizations of Bidirectional Typing}

\cite{xie-higher-rank} review the algorithm from \cite{dunfield-complete-2013} (Sec. 2.3) and propose refinements, including adding an application mode alongside inference and checking (Sec. 3). They also present a novel algorithm for kind inference (Sec. 7) and compare their system to GHC's implementation (Sec. 8.6), identifying potential areas for GHC improvement (Appendix Sec. C).

\cite{xue-contextual-2024} address limitations of traditional bidirectional typing (Sec 2.5) by generalizing it to contextual typing. Instead of propagating only type information, this approach propagates arbitrary contextual information relevant to type checking. It replaces the binary inference/checking modes with counters that track the flow of contextual information. This allows for more fine-grained specification of precisely where programmer annotations are required.

\subsubsection{Beyond Bidirectional Typing: Impredicativity}

\citeauthor{parreaux-when-2024} \cite{parreaux-when-2024} propose SuperF, a novel, non-bidirectional type inference algorithm designed to support first-class (impredicative) higher-rank polymorphism. Impredicative polymorphism permits the instantiation of type variables with polytypes, whereas predicative polymorphism restricts instantiation to monotypes \cite[Sec 3.4]{jones-practical-2007}. SuperF infers a type for each subterm and then checks these against user-provided annotations written in System F syntax. The authors argue that the subtype inference employed by SuperF is better suited for implementing first-class polymorphism than approaches relying on first-order unification. As evidence of its expressive power, the authors demonstrate that SuperF can successfully type a wide variety of terms, often even without explicit type annotations (Sec 5.4, Sec 5.5). The algorithm implementation in Scala is available in \cite{github-hkust-taco-superf-2025}.