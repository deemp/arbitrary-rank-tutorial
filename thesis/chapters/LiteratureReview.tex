\chapter{Literature Review}
\label{chap:LiteratureReview}

We studied several abstract syntax tree representations (\cref{sec:AstRepresentations}) and type inference algorithms (\cref{sec:TypeInferenceAlgorithm}) to understand which ones could be used in different parts of our implementation.

\section{Technical constraints}
\label{chap:LiteratureReview:sec:AstRepresentations:TechnicalConstraints}

\begin{itemize}
    \item We thought about extending our language to support modules, integer and floating-point numbers, and record types in future.
    \item We wanted to be able to freely use mutually recursive data types for different categories of AST nodes. For example, such categories could be modules and statements. A module could contain a number of statements and a statement could introduce a module.
    \item For the language server, we needed to annotate some of the AST nodes with additional information such as locations of the source code that corresponded to these nodes and types of expressions represented by these node subtrees.
    \item We used the BNFC parser generator that generated an AST data type parameterised by an annotation type variable.
          % TODO not only start position - see https://github.com/BNFC/bnfc/pull/463
          After parsing, the annotation of almost each AST node contained the position of a source code span parsed to produce that node.
\end{itemize}

\section{AST representations}
\label{sec:AstRepresentations}

For the AST representation, we could use the data types produced by the parser generator BNFC (\cref{chap:LiteratureReview:sec:AstRepresentations:BnfcAst}), \texttt{hypertypes} (\cref{chap:LiteratureReview:sec:AstRepresentations:Hypertypes}) and alternative representations described in that project, \texttt{compdata} (\cref{chap:LiteratureReview:sec:AstRepresentations:Compdata}), the Free Foil (\cref{chap:LiteratureReview:sec:AstRepresentations:FreeFoil}), or the Trees that grow approach (\cref{chap:LiteratureReview:sec:AstRepresentations:TreesThatGrow}).

\subsection{BNFC AST}
\label{chap:LiteratureReview:sec:AstRepresentations:BnfcAst}

The BNFC-generated parsers did not support parsing signed integer numbers out of the box.
Our workaround was to define a \texttt{token}, then use that \texttt{token} to parse code and construct a node containing a string in the correct format, then post-process the parsed AST to replace nodes containing such raw values with \texttt{internal} (not parsable) nodes containing numbers.

\begin{minted}{haskell}
token IntegerSigned  ('-'? digit+) ;
LitIntegerRaw.       Object ::= IntegerSigned ;
internal LitInteger. Object ::= IntegerSigned ;
\end{minted}

It was inconvenient though that both variants of nodes were in the AST and we would have to pattern-match on the \texttt{LitIntegerRaw} even if that variant of node was completely unnecessary after the post-processing.

\subsection{\texttt{hypertypes}}
\label{chap:LiteratureReview:sec:AstRepresentations:Hypertypes}

The \texttt{hypertypes} package \cite{hypertypes-hackage} let users construct expressions from individual components like in the Data types Ã  la carte \cite{swierstra-data-2008} approach. These components could be mutually recursive types like in \texttt{multirec} \cite{multirec-hackage} and could be processed via type classes. The package description explained the limitations of several previous approaches.

The package provided primitives for annotating nodes (\href{https://hackage.haskell.org/package/hypertypes-0.2.2/docs/Hyper-Combinator-Ann.html}{\texttt{Hyper.Ann}}), for constructing typed lambda calculus expressions (\href{https://hackage.haskell.org/package/hypertypes-0.2.2/docs/Hyper-Syntax.html}{\texttt{Hyper.Syntax}}), and for unification (\href{https://hackage.haskell.org/package/hypertypes-0.2.2/docs/Hyper-Unify.html}{Hyper.Unify}). The \href{https://github.com/lamdu/hypertypes/blob/06cf48ef9c85c54cbe722a448754cb89931b23e7/src/Hyper/Diff.hs}{\texttt{Hyper.Diff}} module showed how to annotate trees and the \href{https://github.com/lamdu/hypertypes/tree/06cf48ef9c85c54cbe722a448754cb89931b23e7/test/TypeLang.hs}{\texttt{TypeLang}} module provided an implementation of type inference for a language with row-types using the primitives provided by the package.

\subsection{\texttt{compdata}}
\label{chap:LiteratureReview:sec:AstRepresentations:Compdata}

Likewise, the \texttt{compdata} package \cite{compdata-hackage} supported mutually recursive data types, including GADTS (\texttt{Data.Comp.Multi}).
Provided examples showed (\href{https://github.com/pa-ba/compdata/blob/e916a9ae847b37d7932669f9365de987d09fd9e0/src/Data/Comp/Multi.hs#L322
}{example1}, \href{https://github.com/pa-ba/compdata/blob/e916a9ae847b37d7932669f9365de987d09fd9e0/examples/Examples/Multi/Desugar.hs}{example2}, \href{https://gist.github.com/liarokapisv/bb857a23ecd9df945690f73e0acfbe80}{example3} related to this \href{https://github.com/pa-ba/compdata/issues/35}{issue}) how to use annotated ASTs.

\section{Stitch}

\citeauthor{eisenberg-stitch-2020} explored an implementation of a simply typed $\lambda$-calculus with a non-typechecked AST with type-level de Bruijn indices that enforced the construction of only well-scoped terms during parsing and a type-checked AST indexed with type-level contexts and node types. The implementation uses lots of Haskell extensions and can be a good playground for studying them in action.

\section{Free Foil}
\label{chap:LiteratureReview:sec:AstRepresentations:FreeFoil}

The Free Foil \cite{kudasov-free-2024} approach by \citeauthor{kudasov-free-2024} implemented in the \cite{free-foil-hackage} package allowed for constructing an AST where nodes were indexed with a phantom type variable denoting the scope. Each node represented either a variable or any other language construct, potentially scoped under a (single) binder that extended the scope. The approach enabled type-safe capture-avoiding substitution of variables for expressions. Additionally, it allowed to define generic recursive AST processing functions that could be used for any AST where nodes were constructed from a user-supplied type (\texttt{sig}) that had necessary type class instances such as \texttt{Bifunctor}.

The example in \href{https://hackage.haskell.org/package/free-foil-0.2.0/docs/Control-Monad-Free-Foil-Example.html}{Control.Monad.Free.Foil.Example} showed a definition of an AST for an untyped lambda calculus. The pattern synonym \href{https://hackage.haskell.org/package/free-foil-0.2.0/docs/src/Control.Monad.Free.Foil.Example.html#LamE}{\texttt{LamE}} was used to construct expressions under a binder.

The Free Foil approach had two limitations that could be inconvenient given our Technical constraints (\cref{chap:LiteratureReview:sec:AstRepresentations:TechnicalConstraints}).

First, the last library version required that mutually recursive types in the AST be combined into a single data type. The library author stated that the library could theoretically support mutually recursive types if another representation were used.

% TODO link to the definition of lexical scoping
Second, it could increase the AST size if our language had modules, or, more generally, supported non-lexical scoping.
In a language with modules, relating the variable declaration and usage sites may require performing multi-phased type checking, e.g., using the scope graphs approach \cite{poulsen-monadic-2023}.
If we wanted to use binders to track the declaration sites, after parsing, we would need to construct an AST with nodes for modules and without binders, then type check that AST, and then construct an AST with resolved binders.
In the subtree of each node that denoted a module import, we would need to create nodes that would introduce binders for visible variables from that module.
At the same time, we would still need to track for each binder the module that the variable came from.

\subsection{Trees That Grow}
\label{chap:LiteratureReview:sec:AstRepresentations:TreesThatGrow}

The GHC used a variant of the Trees That Grow \cite{najd-trees-2016} approach (\href{https://gitlab.haskell.org/ghc/ghc/-/wikis/implementing-trees-that-grow/hs-syn}{HsSyn}, \href{https://gitlab.haskell.org/ghc/ghc/-/wikis/implementing-trees-that-grow/trees-that-grow-guidance}{Trees that grow guidance}).

This approach suggests to parameterise a data type with a type variable and use open type families applied to that type variable instead of concrete types for constructor fields. This way, it is possible to specify a different set type family instances and corresponding field types for each instantiation of the parameter. For example, in GHC, the AST is parameterised by the compilation phase (parsed, renamed, typechecked), and the type families are used to disable certain constructors and choose which nodes should have annotations of particular types.

The data type should also have an additional constructor with a field specified in the similar manner using a type family. Then, it will be possible to "add constructors" to the data type by resolving the type of that field to another data type and providing pattern synonyms that would make constructors of another type wrapped in the constructor of the additional field look as if they belong to the initial data type. A similar approach could be used with constructors of the initial data type to "add" fields to these constructors if they provided an additional field represented as an application of a type family.

Unlike Free Foil, this approach supported mutually recursive types and was similar to using an ordinary data type parameterised by a type variable except one had to define quite a lot of type family instances for constructor fields.

\section{Type inference algorithm}
\label{sec:TypeInferenceAlgorithm}

\subsection{GHC}

% TODO more precise number
The current GHC type inference engine has tens of thousands lines of code.
It has been incrementally developed over years to add new extensions to the Haskell type system.
Some of these extensions had accompanying scientific publications.
% TODO proof that it's based on the paper
One of such extensions was \href{https://gitlab.haskell.org/haskell/prime/-/wikis/RankNTypes}{\texttt{RankNTypes}} that enabled arbitrary-rank predicative polymorphism.
The extension was based on \cite{jones-practical-2007} that described a bidirectional type inference algorithm for a type system with arbitrary-rank types. The Technical Appendix \cite{practical-type-inference-proofs} provided the proofs of theorems mentioned in the paper, including the proofs of completeness and soundness, but not of the attached Haskell implementation.

\subsection{New algorithms}

\cite{jones-practical-2007} was published around 20 years ago, and since then, multiple new type inference algorithms outside of the work on the GHC had been published.
Moreover, some of them were implemented in Haskell \cite{github-goldenberg-artem-goldenbergbidirectionalsystem-2025} \cite{github-choi-kwanghoonbidi-2025} \cite{github-chen-cu1ch3ntype-inference-zoo-2025}.

The following sections
% TODO add links to implementations.

\subsection{Bidirectional typing}

There are two modes in bidirectional typing - the inference mode that helps reduce the number of type annotations required from the programmer and propagates the available type information, and the checking mode that can type expressions for which a type could not be inferred otherwise \cite{dunfield-bidirectional-2020}.

In \cite{dunfield-complete-2013}, \citeauthor{dunfield-complete-2013} provided an arguably simple bidirectional type inference algorithm for a system with higher-rank predicative polymorphism. Their algorithm had similar properties \cite[Fig. 15]{dunfield-complete-2013} as the one presented in \cite[Sec.~6]{jones-practical-2007}, namely, it abided the $\eta$-law \cite[Ch.~4]{selinger-lecture-2013} and was sound and complete wrt. the System F \cite[Ch.~8]{selinger-lecture-2013}. The authors argued that using ordered contexts, existential variables, and precise scoping rules in their work were a better fit from the type-theoretic point of view than using the ``bag of constraints" approach, unification variables, and skolemization that were described, e.g., in \cite{jones-practical-2007}.

In a later work \cite{dunfield-sound-2019}, \citeauthor{dunfield-sound-2019} built on \cite{dunfield-complete-2013} and described the type inference algorithm for a significantly extended language that featured existential quantification, sums, products, pattern matching, etc. For their algorithm and proofs, they used a desugared version \cite[Fig. 11]{dunfield-sound-2019} of the surface language \cite[Fig. 1]{dunfield-sound-2019}.

Additionally, \citeauthor{dunfield-bidirectional-2020} extensively surveyed \cite{dunfield-bidirectional-2020} the works on bidirectional typing and supplied a guide to creating new programmer-friendly bidirectional type inference algorithms.

\subsection{Modifications of bidirectional typing}

\citeauthor{xie-higher-rank} \cite{xie-higher-rank} reviews \cite{dunfield-complete-2013} (Sec. 2.3), suggests using the application mode in addition to inference and checking modes (Sec. 3), provides a novel algorithm for kind inference (Sec. 7), and relates it to the current GHC implementation (Sec. 8.6), noting possible points for improvement (Appendix Sec. C).

\citeauthor{xue-contextual-2024} \cite{xue-contextual-2024} explain the limitations of bidirectional typing (Sec 2.5) and generalize bidirectional typing to contextual typing by propagating any necessary contextual information instead of just type information and replacing the two modes (inference and checking) with counters that track the amount of the contextual information to be propagated. Their approach allows to specify exact places in the code where a programmer must provide annotations.

\subsection{Beyond bidirectional typing}

\citeauthor{parreaux-when-2024} suggest a novel non-bidirectional type inference algorithm SuperF that supports first-class (i.e., impredicative) higher-rank polymorphism. Impredicative polymorphism allows instantiation of type variables with polytypes while predicative allows only monotypes \cite[Sec 3.4]{jones-practical-2007}. The algorithm infers a type for each subterm and then checks against user annotations written in System F syntax. The authors claim that subtype inference used in their approach suits much better for implementing first-class polymorphism than first-order unification-based approaches. As a demonstration of the capabilities of the algorithm, the authors show that it types almost any term even without type annotations (Sec 5.4, Sec 5.5).
