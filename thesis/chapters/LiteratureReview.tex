\chapter{Literature Review}
\label{chap:LiteratureReview}

\section{The Foundations of Typed Functional Languages}
\label{sec:LitReviewFoundations}

At the heart of modern, statically-typed functional languages like Haskell and OCaml lies a sophisticated type system \cite{haskell-type-systems-research, ocaml-papers}, a collection of formal rules assigning a property, or \textbf{type}, to every construct in a program \cite{pierce-types-2002}. A well-typed program is guaranteed to be free from a large class of runtime errors, such as applying an arithmetic operation to a string, a crucial property known as \textbf{type safety}. The theoretical basis for these languages is the \textbf{Lambda Calculus}, a formal system for expressing computation through function abstraction and application.

The most foundational typed variant is the \textbf{Simply-Typed Lambda Calculus (STLC)}. It ensures type safety by requiring every function parameter to be explicitly annotated with a type, and it verifies that functions are only ever applied to arguments of the matching type \cite{Pierce-SF2}. For example, the boolean \texttt{not} function, \texttt{\textbackslash(x : Bool). if x then false else true}, is correctly assigned the type \texttt{Bool -> Bool}.

However, STLC's safety comes at the cost of expressivity. The system is fundamentally \textbf{monomorphic}: each function can only have one, specific type. An identity function, for instance, can be written for booleans (\texttt{\textbackslash(x~:~Bool).~x}) or for integers (\texttt{\textbackslash(x~:~Int).~x}), but a single, universal identity function that works for \textit{all} types is impossible to express. This inherent rigidity makes pure STLC impractical for building reusable, large-scale software and creates a fundamental need for more expressive polymorphic systems.

\section{System F: The Ideal of Polymorphism and its Practical Limits}
\label{chap:LiteratureReview:sec:PolymorphismAndSystemF}

To overcome the limitations of monomorphism, modern languages rely on \textbf{polymorphism}, which allows a single piece of code to operate on values of multiple types. The foundational system that formally introduced this capability is Jean-Yves Girard's \textbf{System F} \cite{girard-system-f}. System F extends the lambda calculus with type abstraction and type application, enabling the creation of truly generic functions through what is known as \textbf{parametric polymorphism}. In System F, a universal identity function can be written with the polymorphic type \texttt{forall a. a -> a}, where the \texttt{forall} quantifier introduces a \textbf{type variable} \texttt{a} that can be instantiated with any concrete type at the function's call site.

Within such powerful systems, a crucial distinction exists between predicative and impredicative polymorphism. In a \textbf{predicative} system, a quantified type variable (like \texttt{a}) can only be instantiated with a \textbf{monotype} --- a simple type that does not itself contain any \texttt{forall} quantifiers. In contrast, a more powerful \textbf{impredicative} system allows a type variable to be instantiated with another \textbf{polytype}. While this "first-class" polymorphism is highly expressive, full type inference for impredicative systems is undecidable \cite{wells-typability-1999, serrano-quick-2020}. This fundamental trade-off between expressive power and decidability directly informs the scope of this thesis, which, following the pragmatic tradition of GHC \cite{jones-practical-2007}, operates within the simpler and more predictable predicative fragment.

\section{Practical Polymorphism: The Hindley-Milner Compromise and the Rank-N Gap}
\label{sec:LitReviewHM}

While System F provides the theoretical ideal for polymorphism, its power makes it impossible to create an algorithm that can always infer a program's types without explicit annotations from the programmer. To make type inference both practical and automatic, languages like ML adopted a decidable and well-behaved subset of System F known as the \textbf{Damas-Hindley-Milner (HM)} type system \cite{damas-milner}.

The practicality of the HM system stems from a foundational compromise: it only supports \textbf{Rank-1} types. This restriction means that \texttt{forall} quantifiers are only permitted at the very outermost level of a type definition. For example, \texttt{forall a. a -> Int} is a valid Rank-1 type, but a type where the quantifier is nested, such as \texttt{(forall a. a -> a) -> Int}, is known as a \textbf{higher-rank type} (specifically, a Rank-2 type) and is forbidden in a standard HM system.

This Rank-1 restriction is the key to the system's tractability, ensuring that type inference is decidable and can be implemented efficiently by algorithms like Algorithm W \cite{jones-practical-2007}. However, this compromise simultaneously introduces a significant expressive limitation: the "Rank-N gap". It becomes impossible to pass a polymorphic function as an argument to another function, as doing so would require a higher-rank type. This "Rank-N gap" represents the central challenge that must be overcome to support many powerful functional programming idioms, motivating the development of more sophisticated, yet still practical, type systems.

\section{A Practical Solution for Higher Ranks}
\label{sec:LitReviewJones2007}

The definitive solution to the Rank-N gap within a practical compiler context was presented in \textit{Practical type inference for arbitrary-rank types} by Peyton Jones et al. \cite{jones-practical-2007}. This seminal paper provides the direct theoretical and architectural foundation for this thesis. Rather than attempting full, undecidable inference, the authors propose a system that extends the predictable Hindley-Milner framework by cleverly leveraging programmer-supplied type annotations to guide the typechecker.

The core contribution is a \textbf{bidirectional type inference algorithm}. The system operates in two modes: an \textbf{inference mode} that synthesizes the most general type for an expression, and a \textbf{checking mode} that verifies an expression against a known, expected type. This duality is the key to its practicality: when a higher-rank type is required (e.g., as a function argument), programmer annotations are used to switch the algorithm into the more constrained checking mode, thereby avoiding the need for full inference while still enabling the use of polymorphic arguments.

To correctly handle the "more polymorphic than" relationship between types, the algorithm introduces a robust \textbf{subsumption} check. The mechanical heart of this check is a technique called \textbf{deep skolemization}, which correctly and efficiently handles quantifiers nested within function types. The detailed mechanics of this algorithm, including the type hierarchy ($\sigma, \rho, \tau$), the monotype invariant for unification, and the contravariant handling of function arguments, will be presented in \Cref{chap:DesignAndMethodology} as they form the direct basis for the implementation in this thesis.

\section{The Modern Landscape of Bidirectional Typing}
\label{chap:LiteratureReview:sec:TypeInferenceAlgorithm}

The bidirectional system from \cite{jones-practical-2007} provides the foundation for this thesis, but the field has continued to evolve. This section surveys key subsequent developments, contextualizing the chosen approach and further justifying its suitability for a pedagogical project. The continued interest in bidirectional typing is evidenced by numerous publicly available implementations, which serve as valuable resources for researchers and students alike \cite{github-goldenberg-artem-goldenbergbidirectionalsystem-2025, github-choi-kwanghoonbidi-2025, github-chen-cu1ch3ntype-inference-zoo-2025}.

\subsection{The Rise of Formal Bidirectional Systems}

A significant thread of modern research has focused on placing bidirectional typing on a more rigorous formal foundation. \citeauthor{dunfield-complete-2013} \cite{dunfield-complete-2013} presented a declarative, bidirectional algorithm for higher-rank predicative polymorphism, proving its soundness and completeness with respect to System F \cite{selinger-lecture-2013}. This foundational work, extended by \citeauthor{dunfield-sound-2019} in \cite{dunfield-sound-2019} and surveyed in \cite{dunfield-bidirectional-2020}, provides a clean type-theoretic basis using ordered contexts, in contrast to the more operational, constraint-based techniques of GHC.

\subsection{Refining and Generalizing Bidirectional Typing}

Subsequent research has focused on refining the bidirectional model. \citeauthor{xie-higher-rank} \cite{xie-higher-rank} proposed refinements to the algorithm from \cite{dunfield-complete-2013}, while more recently, \citeauthor{xue-contextual-2024} \cite{xue-contextual-2024} generalized the approach to \textbf{contextual typing}, allowing for more fine-grained propagation of type information.

\subsection{Exploring Alternatives: Impredicativity and Subtyping}

In parallel, another line of research has explored supporting full, first-class impredicativity. \citeauthor{parreaux-when-2024} \cite{parreaux-when-2024} propose \textbf{SuperF}, a novel algorithm that supports impredicativity by replacing unification with \textbf{subtype inference}\footnote{The SuperF implementation can be found at \url{https://github.com/hkust-taco/superf}.}. While extremely powerful, this approach represents a significant departure from the unification-based systems that are the focus of this thesis.

\subsection{Justification of the Chosen Approach}

This survey confirms that while many innovative approaches exist, the practical, predicative, and unification-based system pioneered in \cite{jones-practical-2007} remains the most suitable foundation for this project. The formal systems of Dunfield et al. are elegant but are further removed from the concrete implementation techniques (like constraint solving and mutable metavariables) used in GHC. The subtyping-based approach of SuperF is a different paradigm entirely.

Therefore, for the pedagogical goals of this thesis --- to create a tutorial implementation that demystifies a production-grade approach --- basing the work on the established and practical foundation of Jones et al. is the most direct and effective strategy. It provides a clear path to understanding the core challenges of higher-rank types and the architectural patterns used to solve them in a real-world compiler.